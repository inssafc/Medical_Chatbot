# -*- coding: utf-8 -*-
"""datapreparation+model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dvZ5Xwa28jV5onhluH0ZRrofWOzaowpY

# **Importing libraries and loading data**
"""

import json
import nltk
from nltk.corpus import stopwords
import string
from nltk.stem import WordNetLemmatizer
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation,Flatten
from sklearn.model_selection import train_test_split
import numpy as np
from keras.optimizers import SGD
import pickle

nltk.download('wordnet')

nltk.download('stopwords')

nltk.download('punkt')

intents = json.loads(open('/content/intents2.json').read())

words = []
classes = []
documents = []
ignore_letters = ['?',',','.','!','s',')','(',' ']

"""# **remove stop wrods, ?, !,...... ignore_letters from words**"""

def preprocess_text(words):
    # Remove stop words
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word.lower() not in stop_words]

    # Remove single letters
    words = [word for word in words if len(word) > 1]

    # Remove punctuation marks
    words = [word for word in words if word not in string.punctuation]

    # Remove additional characters like "!", "?"
    words = [word for word in words if word not in ignore_letters]

    return words

tags = []
for intent in intents['intents']:
  for pattern in intent['patterns']:
    word_list = nltk.word_tokenize(pattern)
    word_list = preprocess_text(word_list)
    words.extend(word_list)
    documents.append((word_list, intent['tag']))
    tags.append(intent['tag'])
    if intent['tag'] not in classes:
      classes.append(intent['tag'])

words = sorted(set(words))
classes=sorted(set(classes))

len(classes)

#inserting list of words and classes into pickle file 
#performing serialization
pickle.dump(words,open('words.pkl','wb'))
pickle.dump(classes,open('classes.pkl','wb'))

from google.colab import files

files.download('words.pkl')
files.download('classes.pkl')

"""# **Encoding data ( converting data into a numerical representation or vectorized form.)**"""

# Create a WordNet lemmatizer object
lemmatizer = WordNetLemmatizer()

#bag of words
x = []
for document in documents:
  bag = []
  word_patterns = document[0]
  word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]
  for word in words:
    bag.append(1) if word in word_patterns else bag.append(0)
  x.append(bag)

len(x)

len(x[0])

#one hot encoding for our classes
from sklearn.preprocessing import OneHotEncoder
# Create an instance of the OneHotEncoder
encoder = OneHotEncoder(categories=[classes])
# Reshape the tags list to a 2D array
tags_2d = [[tag] for tag in tags]
# Apply one-hot encoding
y_encoded = encoder.fit_transform(tags_2d)
# Convert the sparse matrix to a dense array
y_encoded_array = y_encoded.toarray()
y = y_encoded_array
# Print the resulting one-hot encoded array

y[0]

len(y[0])

"""# **Data splitting**"""

train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=40)

"""# **Model training**"""

len(train_x[0])

model = Sequential()
model.add(Dense(256,input_shape=(len(train_x[0]),),activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(128,activation='sigmoid'))
model.add(Dropout(0.2))
model.add(Dense(len(train_y[0]),activation='softmax'))

sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer = sgd, metrics= ['accuracy'])

hist = model.fit(np.array(train_x), np.array(train_y), epochs=400, verbose=1, batch_size=16)

res = model.predict(np.array([test_x[0]]))

len(x[5])

res

model.save('chatbotmodel.h5',hist)

!pip install h5py

import shutil

source_path = 'chatbotmodel.h5'
destination_path = r'C:\Users\DELL\Documents\chatbotfiles\chatbotmodel.h5'

shutil.copy(source_path, destination_path)